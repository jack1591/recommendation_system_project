# Рекомендательная система для проекта в `karpov.courses`
Система, которая выдает топ-5 рекомендации постов пользователям.

## Описание
Представим, что существует социальная сеть, которая обладает следующим функционалом: можно отправлять друг другу письма, создавать сообщества, аналогичные группам в известных сетях, и в этих сообществах публиковать посты.  
* При регистрации студенты должны заполнять данные по своему профилю, которые хранятся в поднятой на наших мощностях postgres database.  
* Также платформа обладает лентой, которую пользователи могут листать и просматривать случайные записи случайных сообществ. Если пост нравится, можно поддержать автора и поставить like.  
* Все действия пользователей сохраняются, каждая их активность, связанная с просмотром постов, тоже записывается в базу.

Платформа заинтересована в благосостоянии пользователей, поэтому разработчики решили усовершенствовать текущую ленту. А что, если показывать пользователям не случайные посты, а рекомендовать их точечно каждому пользователю из всего имеющегося множества написанных постов? Как это сделать и учесть индивидуальные характеристики профиля пользователя, его прошлую активность и содержимое самих постов?  

**Необходимо реализовать сервис, который будет для каждого юзера в любой момент времени возвращать посты, которые пользователю покажут в его ленте соцсети.**  

## Метрика
Качество написанного алгоритма проверяется по скрытому ряду user_id и ряду timestаmp (эмулируем запросы пользователей в разное время) по метрике **hitrate@5**:  
  
$$\frac{1}{n*T} \sum_{t=1}^{T} \sum_{i=1}^{n} \min(1, \sum_{j=1}^{5} [\mathbb{a}_{j}(\mathbb{x}_{i}, t) = 1])$$  

где:
* n - количество юзеров
* T - количество периодов проверки
* $$a_j(x_i, t)$$ - j-ая рекомендация i-ому пользователю в момент времени t;

_**Пояснение**_: Если из 5 рекомендаций хотя бы 1 оказалась релевантна для пользователя (подошла ему), значение будет равно 1, иначе 0 (бинарная метрика).

## Данные

**Таблица user_data**  
Cодержит информацию о всех пользователях соц.сети  
| Field Name | Overview |
|:-----------|:---------|
| age | Возраст пользователя (в профиле) |
| city | Город пользователя (в профиле) |
| country | Страна пользователя (в профиле) |
| exp_group | Экспериментальная группа: некоторая зашифрованная категория |
| gender | Пол пользователя |
| user_id | Уникальный идентификатор пользователя |
| os | Операционная система устройства, с которого происходит пользование соц.сетью |
| source | Пришел ли пользователь в приложение с органического трафика или с рекламы |
  
**Таблица post_text_df**  
Содержит информацию о постах и уникальный ID каждой единицы с соответствующим ей текстом и топиком  
| Field name | Overview |
|:-----------|:---------|
|id|	Уникальный идентификатор поста|
|text|	Текстовое содержание поста|
|topic|	Основная тематика|
  
**Таблица feed_data**  
Содержит историю о просмотренных постах для каждого юзера в изучаемый период.  
_**Данная таблица очень большая, я обучал модель на 500,000 записей**_
| Field name | Overview |
|:-----------|:---------|
|timestamp|	Время, когда был произведен просмотр|
|user_id|	id пользователя, который совершил просмотр|
|post_id|	id просмотренного поста|
|action|	Тип действия: просмотр или лайк|
|target|	1 у просмотров, если почти сразу после просмотра был совершен лайк, иначе 0. У действий like пропущенное значение|

## Обучение модели
### Предобработка данных
1. Убрал строки с лайками у пользователей, т.к. они являются избыточными. Тем самым уменьшил размер датафрейма feed_data на **10 процентов**  
2. Сформировал кластеры на основе текстов постов (признак text в таблице post_text_df)  
   2.1. Посчитал TF-IDF для каждого слова каждого поста (получил около 40 тысяч новых признаков)  
   2.2. С помощью PCA уменьшил размерность с **40 000** до **20**  
   2.3. С помощью kmeans посчитал расстояние от каждого поста до каждого кластера (сформировал 15 кластеров), тем самым получил что-то наподобие категорий для постов  
3. Добавил признаки month, day, hour из временного признака timestamp в таблице **feed_data**
4. Удалил признаки text и action, так как они стали избыточными
5. Объединил датафреймы для обучения модели

### Разделение тренировочной и тестовой выборок
Так как мы будем предлагать пользователям уже существующие посты, но только в будущем, основываясь на их активностях в прошлом, я разделил данные на тренировочные и тестовые по значениям признака timestamp в соотношении примерно 5:1.
```
df_train = df[df['timestamp'] < '2021-12-15']
df_test = df[df['timestamp'] >= '2021-12-15']
```

### Выбор модели
В моей рекомендательной системе используется бустинг CatBoostClassifier, который построен на 500 деревьях глубины 2 и шагом обучения 1.

## Описание структуры проекта
- **entities/** - Модели данных для представления таблиц
  - `schema.py` - Основные модели
  - `table_feed.py` - Класс с информацией об активностях пользователей
  - `table_post.py` - Класс постов
  - `table_user.py` - Класс пользователей
- **model_creating/** - Директория с обучением модели
  - **catboost_model** - Обученная модель
  - `practice_second.ipynb` - Ноутбук с обучением модели
- **README.md** - Основная документация проекта
- `app.py` - Главный файл, содержит все эндпоинты
- `database.py` - Настройки и подключение к базе данных
- `recommendation_endpoint.py` - эндпоинт для системы рекомендаций
- **requirements.txt** - Список зависимостей Python
- `test_client.py` - Тест с информацией о конкретном пользователе

### Инструкции по запуску
1. `gin clone https://github.com/jack1591/recommendation_system_project.git` - клонирование репозитория себе на компьютер  
2. `pip install -r requirements.txt` - загрузка всех зависимостей проекта  
3. `python test_client.py` - запуск файла с запросом 5 рекомендаций для конкретного пользователя в конкретный момент времени  

Запуск файлов для самостоятельной проверки эндпоинтов (через Postman или просто через localhost): `uvicorn app:app --reload`
